{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from operator import add\n",
    "from datetime import datetime, timedelta\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "packages = \"org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.1\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages {0} pyspark-shell\".format(packages)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ambiente(line):\n",
    "    line = line.replace('\"', '')\n",
    "    s = line.split(\",\")\n",
    "    try:\n",
    "        return [{\n",
    "                #\"date\": str(s[1]),\n",
    "                \"date\": datetime.strptime(s[1], \"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"Temperature\": float(s[2]), #\n",
    "                \"Humidity\": float(s[3]), #\n",
    "                \"Light\": float(s[4]),\n",
    "                \"CO2\": float(s[5]), #\n",
    "                \"HumidityRatio\": float(s[6]),\n",
    "                \"Occupancy\": int(s[7])\n",
    "              }\n",
    "             ]\n",
    "    except Exception as err:\n",
    "        print(\"Wrong line format (%s): \" % line)\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_from_start(new_values, actual_values): \n",
    "    if actual_values is None:\n",
    "        actual_values = (0, 0)\n",
    "    new_values = new_values[0]\n",
    "    new_sum = new_values[0] + actual_values[0]\n",
    "    new_count = new_values[1] + actual_values[1]\n",
    "    return new_sum, new_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_interval(current, preceding):\n",
    "    print(current)\n",
    "    current = current[0]\n",
    "    print(current)\n",
    "    if preceding is None:\n",
    "        interval = 0\n",
    "    else:\n",
    "        print(preceding)\n",
    "        interval = (current-preceding).total_seconds()\n",
    "        print(interval)\n",
    "    return interval, current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_avg_per_batch(rdd):\n",
    "    elems = list(rdd.collect())\n",
    "    with open('data/output/all-average-batch-log.csv', 'a') as file:\n",
    "        for record in elems:    \n",
    "            file.write(str(record) + \"\\n\")\n",
    "\n",
    "def save_all_avg_cummulated(rdd):\n",
    "    elems = list(rdd.collect())\n",
    "    with open('data/output/all-average-cummulated-log.csv', 'a') as file:\n",
    "        for record in elems:    \n",
    "            file.write(str(record) + \"\\n\")\n",
    "            \n",
    "def save_light_avg_windowed(rdd):\n",
    "    elems = list(rdd.collect())\n",
    "    with open('data/output/light-average-windowed-log.csv', 'a') as file:\n",
    "        for record in elems:    \n",
    "            file.write(str(record) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Calcular el promedio de valores de temperatura, humedad relativa y concentración de CO2 para cada micro-batch, y el promedio de dichos valores desde el arranque de la aplicación.\n",
    "\n",
    "#### a) promedio para cada micro-batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"PracticaSparkStreaming\")\n",
    "micro_batch_duration = 5\n",
    "ssc = StreamingContext(sc, micro_batch_duration)\n",
    "kafkaParams = {\"metadata.broker.list\": \"localhost:9092\"}\n",
    "directKafkaStream = KafkaUtils.createDirectStream(ssc, [\"test\"], kafkaParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = directKafkaStream.map(lambda o: str(o[1]))\n",
    "parsed_lines = lines.flatMap(parse_ambiente).map(lambda o: (o[\"Temperature\"], o[\"Humidity\"], o[\"CO2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el cálculo de la media busco calcular la suma y el count por separado y guardar ambos en una tupla que comparte una misma clave por ejemplo \"t\". Luego hago un join de ambos por esta clave y finalmente la tiro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sum = parsed_lines.map(lambda x: x[0]).reduce(lambda x, y: (x + y)).map(lambda x: (\"t\", x))\n",
    "t_count = parsed_lines.count().map(lambda x: (\"t\", x))\n",
    "t_tuple = t_sum.join(t_count)\n",
    "t_avg = t_tuple.map(lambda x: x[1]).map(lambda x: (\"avg temperature\", x[0]/x[1]))\n",
    "\n",
    "h_sum = parsed_lines.map(lambda x: x[1]).reduce(lambda x, y: (x + y)).map(lambda x: (\"h\", x))\n",
    "h_count = parsed_lines.count().map(lambda x: (\"h\", x))\n",
    "h_tuple = h_sum.join(h_count)\n",
    "h_avg = h_tuple.map(lambda x: x[1]).map(lambda x: (\"avg humidity\", x[0]/x[1]))\n",
    "\n",
    "co2_sum = parsed_lines.map(lambda x: x[2]).reduce(lambda x, y: (x + y)).map(lambda x: (\"co2\", x))\n",
    "co2_count = parsed_lines.count().map(lambda x: (\"co2\", x))\n",
    "co2_tuple = co2_sum.join(co2_count)\n",
    "co2_avg = co2_tuple.map(lambda x: x[1]).map(lambda x: (\"avg CO2\", x[0]/x[1]))\n",
    "\n",
    "all_avg = t_avg.union(h_avg).union(co2_avg)\n",
    "all_avg.foreachRDD(save_all_avg_per_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) promedio desde el arranque de la aplicación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"PracticaSparkStreaming\")\n",
    "micro_batch_duration = 5\n",
    "ssc = StreamingContext(sc, micro_batch_duration)\n",
    "kafkaParams = {\"metadata.broker.list\": \"localhost:9092\"}\n",
    "directKafkaStream = KafkaUtils.createDirectStream(ssc, [\"test\"], kafkaParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = directKafkaStream.map(lambda o: str(o[1]))\n",
    "parsed_lines = lines.flatMap(parse_ambiente).map(lambda o: (o[\"Temperature\"], o[\"Humidity\"], o[\"CO2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el calculo del promedio acumulado uso un updateStateByKey, para lo cual también necesito una clace adicional. Luego dentro de la función de update la tiro esa clave. la función de update es sencilla y devuelve el count y la suma acumulados. La división la hago fuera con un map, dado que ahora ambos son parte del mismo DStream. El problema que le veo es que los valores del estado acumulados crecen y no sé si eso puede reducir eficiencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sum = parsed_lines.map(lambda x: x[0]).reduce(lambda x, y: (x + y)).map(lambda x: (\"t\", x))\n",
    "t_count = parsed_lines.count().map(lambda x: (\"t\", x))\n",
    "t_tuple = t_sum.join(t_count)\n",
    "\n",
    "h_sum = parsed_lines.map(lambda x: x[1]).reduce(lambda x, y: (x + y)).map(lambda x: (\"h\", x))\n",
    "h_count = parsed_lines.count().map(lambda x: (\"h\", x))\n",
    "h_tuple = h_sum.join(h_count)\n",
    "\n",
    "co2_sum = parsed_lines.map(lambda x: x[2]).reduce(lambda x, y: (x + y)).map(lambda x: (\"co2\", x))\n",
    "co2_count = parsed_lines.count().map(lambda x: (\"co2\", x))\n",
    "co2_tuple = co2_sum.join(co2_count)\n",
    "\n",
    "sc.setCheckpointDir(\"data/checkpoint/\")\n",
    "\n",
    "cummulated = t_tuple.updateStateByKey(update_from_start)\n",
    "t_avg_actual = cummulated.map(lambda x: x[1]).map(lambda x: (\"avg cummulated temperature\", x[0]/x[1]))\n",
    "\n",
    "cummulated = h_tuple.updateStateByKey(update_from_start)\n",
    "h_avg_actual = cummulated.map(lambda x: x[1]).map(lambda x: (\"avg cummulated humidity\", x[0]/x[1]))\n",
    "\n",
    "cummulated = co2_tuple.updateStateByKey(update_from_start)\n",
    "co2_avg_actual = cummulated.map(lambda x: x[1]).map(lambda x: (\"avg cummulated CO2\", x[0]/x[1]))\n",
    "\n",
    "all_avg_actual = t_avg_actual.union(h_avg_actual).union(co2_avg_actual)\n",
    "all_avg_actual.foreachRDD(save_all_avg_cummulated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Calcular el promedio de luminosidad en la estancia en ventanas deslizantes de tamaño 45 segundos, con un valor de deslizamiento de 15 segundos entre ventanas consecutivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"PracticaSparkStreaming\")\n",
    "micro_batch_duration = 5\n",
    "ssc = StreamingContext(sc, micro_batch_duration)\n",
    "kafkaParams = {\"metadata.broker.list\": \"localhost:9092\"}\n",
    "directKafkaStream = KafkaUtils.createDirectStream(ssc, [\"test\"], kafkaParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = directKafkaStream.map(lambda o: str(o[1]))\n",
    "parsed_lines = lines.flatMap(parse_ambiente).map(lambda o: o[\"Light\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir(\"data/checkpoint/\")\n",
    "l_sum = parsed_lines.reduceByWindow(lambda x, y: x+y, lambda x, y: x-y, 45, 15).map(lambda x: (\"l\", x))\n",
    "l_count = parsed_lines.countByWindow(45, 15).map(lambda x: (\"l\", x))\n",
    "l_tuple = l_sum.join(l_count)\n",
    "l_avg_window = l_tuple.map(lambda x: x[1])\\\n",
    "                    .map(lambda x: x[0]/x[1])\\\n",
    "                    .map( lambda x: 'avg light windowed %s' % x)\n",
    "l_avg_window.foreachRDD(save_light_avg_windowed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3. Examinando los datos, podemos apreciar que el intervalo entre muestras originales no es exactamente de 1 minuto en muchos casos. Calcular el número de parejas de muestras consecutivas en cada micro-batch entre las cuales el intervalo de separación no es exactamente de 1 minuto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Fallida] Mi idea es usar la función reduce que hace una operación de 2 a 1 de forma escalonada, es decir, el 1 con el 2, luego el resultado con el 3, etc. Pero quizá sea la aproximación erronea. El caso es que da errores de tipo al operar con datetime, y el reduce de un lambda normal me daba error.\n",
    "\n",
    "Una vez obtenidos los intervalos entre cada muestra, haría un filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"PracticaSparkStreaming\")\n",
    "micro_batch_duration = 5\n",
    "ssc = StreamingContext(sc, micro_batch_duration)\n",
    "kafkaParams = {\"metadata.broker.list\": \"localhost:9092\"}\n",
    "directKafkaStream = KafkaUtils.createDirectStream(ssc, [\"test\"], kafkaParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = directKafkaStream.map(lambda o: str(o[1]))\n",
    "parsed_lines = lines.flatMap(parse_ambiente).map(lambda o: o[\"date\"])\n",
    "parsed_lines.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir(\"data/checkpoint/\")\n",
    "interval = parsed_lines.reduce(calc_interval)\n",
    "interval.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_lines.reduce(lambda x, y: x-y).pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-07-15 23:22:50\n",
      "-------------------------------------------\n",
      "2015-02-05 08:43:00\n",
      "2015-02-05 08:44:00\n",
      "2015-02-05 08:44:59\n",
      "2015-02-05 08:45:59\n",
      "2015-02-05 08:47:00\n",
      "2015-02-05 08:48:00\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-07-15 23:22:55\n",
      "-------------------------------------------\n",
      "2015-02-05 08:49:00\n",
      "2015-02-05 08:50:00\n",
      "2015-02-05 08:51:00\n",
      "2015-02-05 08:51:59\n",
      "2015-02-05 08:53:00\n",
      "2015-02-05 08:54:00\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-07-15 23:23:00\n",
      "-------------------------------------------\n",
      "2015-02-05 08:55:00\n",
      "2015-02-05 08:55:59\n",
      "2015-02-05 08:57:00\n",
      "2015-02-05 08:57:59\n",
      "2015-02-05 08:58:59\n",
      "2015-02-05 09:00:00\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-07-15 23:23:05\n",
      "-------------------------------------------\n",
      "2015-02-05 09:01:00\n",
      "2015-02-05 09:02:00\n",
      "2015-02-05 09:03:00\n",
      "2015-02-05 09:04:00\n",
      "2015-02-05 09:04:59\n",
      "2015-02-05 09:06:00\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-07-15 23:23:10\n",
      "-------------------------------------------\n",
      "2015-02-05 09:07:00\n",
      "2015-02-05 09:08:00\n",
      "2015-02-05 09:08:59\n",
      "2015-02-05 09:10:00\n",
      "2015-02-05 09:10:59\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-07-15 23:23:15\n",
      "-------------------------------------------\n",
      "2015-02-05 09:11:59\n",
      "2015-02-05 09:13:00\n",
      "2015-02-05 09:14:00\n",
      "2015-02-05 09:15:00\n",
      "2015-02-05 09:16:00\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-07-15 23:23:20\n",
      "-------------------------------------------\n",
      "2015-02-05 09:16:59\n",
      "2015-02-05 09:17:59\n",
      "2015-02-05 09:19:00\n",
      "2015-02-05 09:20:00\n",
      "2015-02-05 09:21:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
